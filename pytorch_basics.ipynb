{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([i for i in range(1,10,2)])\n",
    "t2 = torch.tensor([i for i in range(1,20,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5,  9, 13, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  8, 14, 20, 26])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  15,  45,  91, 153])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1*t2  # elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1,2,5,6,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([146])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.matmul(t2.T) # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.5000, 1.0000, 1.1667, 1.1250]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  7,  9, 11, 13])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add_(2*2)  # If tensor stored in CPU, inplace operations to a tensor will share common memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7,  9, 11, 13], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # checks if gpu available \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    z = x **3\n",
    "    z = z.to(device) # moves to gpu\n",
    "\n",
    "    y = x+z # runs on gpu\n",
    "\n",
    "    y.numpy() ## ERROR, numpy can only handle CPU tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of m : 3.0\n",
      "Grad of c : 1.0\n"
     ]
    }
   ],
   "source": [
    "# regression slope form\n",
    "# y = mx + c\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "\n",
    "m = torch.tensor(1.0, requires_grad=True)\n",
    "c = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "y = m*x + c\n",
    "\n",
    "y.backward() # dy/dx = m + 1 \n",
    "print(f'Grad of m : {m.grad}') # --> 3\n",
    "print(f'Grad of c : {c.grad}') # --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7260,  0.2031,  0.2999,  1.3439,  0.9924], requires_grad=True)\n",
      "tensor([-0.7260,  0.2031,  0.2999,  1.3439,  0.9924])\n",
      "tensor([-0.7260,  0.2031,  0.2999,  1.3439,  0.9924])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "z = x.requires_grad_(False) ## setting the grad to tensor\n",
    "print(z)\n",
    "\n",
    "y = x.detach() ## new tensor detaching the grad functn\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1145,  5.0146,  8.0146, 10.0146, 12.0146])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([.1,2,5,7,9], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x + 2.03*1.485\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(5):  # Training loop\n",
    "    model_output = (weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_() # setting the grad zero for every loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```x ---> a(x) ---->y b(y) ----> z```\n",
    "\n",
    "**Chain Rule:**\n",
    "Gradient -> dz/dx = dz/dy*dy/dx \n",
    "\n",
    "```Computational Graph```\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* 3 steps process:\n",
    "1. Forward pass: Compute loss\n",
    "2. Compute local gradients\n",
    "3. Backward pass: Compute ```d(Loss)/d(Weights)``` using chain rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "## Example \n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass & calc loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can build any model by 4 ways:\n",
    "\n",
    "* case: 1\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Manually\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 2\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 3\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "\n",
    "* case: 4\n",
    "    * Prediction: PyTorch Model\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 9.99999977350235\n"
     ]
    }
   ],
   "source": [
    "# f = w*x\n",
    "# f = 2*x\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "## Gradient \n",
    "## MSE          =    1/N *  (w*x - y)**2\n",
    "## d(MSE)/dw    =    1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    dw = gradient(X,Y,y_pred) # gradients\n",
    "\n",
    "    w -= learning_rate * dw  # update weights \n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=0.01)\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "prediction before training: f(5) = -2.577\n",
      "epoch 1: w = -0.134, loss = 47.54420471\n",
      "epoch 11: w = 1.102, loss = 2.40400505\n",
      "epoch 21: w = 0.490, loss = 5.51834106\n",
      "epoch 31: w = 0.437, loss = 3.22923541\n",
      "epoch 41: w = 0.888, loss = 1.88684821\n",
      "epoch 51: w = 1.324, loss = 0.74377757\n",
      "epoch 61: w = 1.557, loss = 0.26913342\n",
      "epoch 71: w = 1.661, loss = 0.15029001\n",
      "epoch 81: w = 1.732, loss = 0.09423008\n",
      "epoch 91: w = 1.800, loss = 0.05414252\n",
      "prediction after training: f(5) = 9.603\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(f'prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = model(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 1071.6234\n",
      "epoch: 20, loss = 697.7000\n",
      "epoch: 30, loss = 454.6804\n",
      "epoch: 40, loss = 296.6356\n",
      "epoch: 50, loss = 193.7850\n",
      "epoch: 60, loss = 126.8075\n",
      "epoch: 70, loss = 83.1607\n",
      "epoch: 80, loss = 54.6972\n",
      "epoch: 90, loss = 36.1218\n",
      "epoch: 100, loss = 23.9904\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGdCAYAAADnrPLBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1dUlEQVR4nO3de3xU9Z3/8fdhMOGaCCTmwgShim0VtQrVgs2SlIq1rYJRVNh18VHFC16IVKlIq6AFqqiAWkRXV+xWwYpB2tq60P4AYxFFKiviDRVMCAl3E7CY4OT8/jjkMpkzycxkZs6cmdfz8ZhHku+cOfNlSs2bz/dmmKZpCgAAwKW6ON0BAACAziDMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAV+vqdAfiobGxUbt27VLv3r1lGIbT3QEAACEwTVOHDh1Sfn6+unQJXn9JiTCza9cuFRQUON0NAAAQgcrKSnm93qDPp0SY6d27tyTrw8jIyHC4NwAAIBR1dXUqKCho/j0eTEqEmaahpYyMDMIMAAAu09EUESYAAwAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAV0uJTfMAAEAM+HxSeblUXS3l5UmFhZLHE/duEGYAAED4ysqkKVOknTtb2rxeaeFCqaQkrl1hmAkAAISnrEy67DL/ICNJVVVWe1lZXLtDmAEAAKHz+ayKjGkGPtfUVlpqXRcnhBkAABC68vLAikxrpilVVlrXxQlhBgAAhK66OrrXRQFhBgAAhC4vL7rXRQFhBgAAhK6w0Fq1ZBj2zxuGVFBgXRcnhBkAABA6j8dafi0FBpqmnxcsiOt+M4QZAAAQnpISaflyqX9//3av12qP8z4zbJoHAADCV1IijRnDDsAAAMDFPB6pqMjpXjDMBAAA3I0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAXI0wAwAAInL4sJSeLg0YIG3Z4lw/CDMAACBsixZJvXtLDQ1SZaVUUeFcX7o699YAAMBtDhyQ+vXzb7v0UunHP3amPxKVGQAAEKI5cwKDzAcfSMuXS4bhTJ8kKjMAAKADNTVSXp5/26RJ0pNPOtOftqjMAACAoKZNCwwyO3YkTpCRCDMAAMDGjh3W0NG8eS1td94pmaZ04omOdcsWw0wAAMDPtddKTz/t31ZdLeXmOtOfjlCZAQAAkqzJvIbhH2R+8xurGpOoQUaiMgMAQMozTWt59YoV/u0HDkh9+jjTp3BQmQEAIIVt2iR16eIfZBYvtgKOG4KMRGUGAICUZJrSyJFSeXlLW5cuUm2t1KuXc/2KBJUZAABSTHm5FVxaB5nnnpN8PvcFGYnKDAAAKcPnk848U9q6taUtO9s6Wyk93bl+dRaVGQAAUsBf/yp17eofZP70J2nPHncHGYnKDAAASa2hwdrkrqampe1b35K2bLHCTTKgMgMAQJL6wx+sqkvrILNmjbWfTLIEGYnKDAAASedf/5KOP146erSlbcSIlom/ySamf6TXXntNF110kfLz82UYhl5++WW/503T1MyZM5Wfn6/u3burqKhIW1sP5kmqr6/XLbfcoqysLPXs2VMXX3yxdu7cGctuAwDgWk89JfXs6R9k3npL+sc/kjPISDEOM19++aXOPPNMPfbYY7bPP/DAA3r44Yf12GOPaePGjcrNzdX555+vQ4cONV9TWlqqFStWaNmyZXr99dd1+PBh/fSnP5XP54tl1wEAcJUvvrCOIpg0qaXt4oulxkbpu991rFtxYZimacbljQxDK1as0NixYyVZVZn8/HyVlpbqF7/4hSSrCpOTk6P7779f119/vWpra5Wdna3/+Z//0RVXXCFJ2rVrlwoKCvSXv/xFF1xwQUjvXVdXp8zMTNXW1iojIyMmfz4AAJzy4IPSHXf4t733nnTaac70J1pC/f3tWMFp+/btqqmp0ejRo5vb0tPTNXLkSK1fv16StGnTJh09etTvmvz8fA0ZMqT5Gjv19fWqq6vzewAAkGz27LGqMa2DzMSJ1u6+bg8y4XAszNQcm1qdk5Pj156Tk9P8XE1NjdLS0tSnzeEQra+xM3fuXGVmZjY/CgoKotx7AACc9atfSW1+herTT6UlSxzpjqMcnwpkGIbfz6ZpBrS11dE106dPV21tbfOjsrIyKn0FAMBplZVWNebXv25pmzrVqsZ84xvO9ctJjoWZ3NxcSQqosOzZs6e5WpObm6uGhgYdPHgw6DV20tPTlZGR4fcAAMDtbrpJGjDAv62qSnroIWf6kygcCzODBg1Sbm6uVq9e3dzW0NCgdevWacSIEZKkoUOH6rjjjvO7prq6Wu+9917zNQAAJLuPP7aqMYsWtbTde69VjcnPd65fiSKmm+YdPnxYn3zySfPP27dv1+bNm9W3b18NGDBApaWlmjNnjgYPHqzBgwdrzpw56tGjhyZMmCBJyszM1DXXXKOf//zn6tevn/r27avbb79dp59+un74wx/GsusAACSE8eOlZcv82/btk/r1c6Y/iSimYebtt99WcXFx889Tp06VJE2cOFFLlizRtGnTdOTIEU2ePFkHDx7Uueeeq1WrVql3797Nr5k/f766du2qyy+/XEeOHNGoUaO0ZMkSeTyeWHYdAABH/d//Sd/5jn/bI49It9ziSHcSWtz2mXES+8wAANzCNKXRo6W//c2/va5OavVv/ZSQ8PvMAAAAf+vXW0cOtA4yzz5rBZxUCzLh4KBJAAAc5vNJ55wj/fOfLW0ZGdZp1927O9cvt6AyAwCAg1avlrp29Q8yZWVSbS1BJlRUZgAAcMDRo9LgwdLnn7e0DRokffSRdNxxzvXLjajMAAAQZ2VlUlqaf5BZvVr67DOCTCSozAAAECdHjkgnnCAdPtzSNmyY9Oab1sRfRIaPDgCAOHj2WalHD/8gs369tHEjQaazqMwAABBDdXVSZqZ/2+jR0quvWkcUoPPIggAAxMgjjwQGmf/7P+l//5cgE01UZgAAiLJ9+6TsbP+28eOl5593pj/JjsoMAABRdO+9gUHm448JMrFEZQYAgCjYtUvq39+/7aabpMcec6Y/qYTKDAAAnTR1amCQqaggyMQLYQYAgAh9+qk1kXf+/Ja2X/3KOhiyoMC5fqUahpkAAIjAxInS737n37ZnT+B8GcQelRkAAMKwdatVjWkdZB580KrGEGScQWUGAIAQmKZ08cXSn//s3/7FF4F7ySC+qMwAANCBt96yjhxoHWSeesoKOAQZ51GZAQAgiMZG6bzzpA0bWtrS06UDB6xzlpAYqMwAAGBjzRrJ4/EPMi+8IH31FUEm0VCZAQCgla+/lk47zdq1t0lenrRjh5SW5li30A4qMwAAHPPnP0vHHecfZP7yF2t3X4JM4qIyAwBIefX11g6++/e3tJ1+uvTOO9ZQExIblRkAQEp7/nmpWzf/IPPaa9K77xJk3ILKDAAgJR0+LPXu7d82cqQ18dcwnOkTIkNlBgCQchYvDgwymzZJa9cSZNyIygwAIGUcPCj17evfduml0osvEmLcjMoMAMA5Pp9VDlm61Prq88XsrX7zm8Ag88EH0vLlBBm3ozIDAHBGWZk0ZYq0c2dLm9crLVwolZRE7W1qaqx9Ylq79lrpv/4ram8Bh1GZAQDEX1mZdNll/kFGkqqqrPaysqi8zZ13BgaZ7dsJMsmGMAMAiC+fz6rImGbgc01tpaWdGnL6/HNr6Oj++1vapk2zbj9wYMS3RYIizAAA4qu8PLAi05ppSpWV1nURuO66wMBSXe0fbJBcCDMAgPiqro7udcd8+KFVjWk9hDRnjpWNcnPDuhVchgnAAID4ajuJpZPXmaY0bpz00kv+7QcOSH36hNk3uBKVGQBAfBUWWquWgq2HNgypoMC6rgP//KfUpYt/kFm0yAo4BJnUQZgBAMSXx2Mtv5YCA03TzwsWtHswkmlKxcXS0KH+Lz10SLrxxuh2F4mPMAMAiL6ONsMrKbF2q+vf37/d67Xa29lnprzcqsasXdvS9vvfS42NUq9e0foDwE2YMwMAiK5QN8MrKZHGjLHSSXW1NUemsDBoRcbnk846S9qypaWtXz/rbbp1i9GfBa5AZQYAEB0+n3TvvdZhR6FuhufxSEVF0vjx1tcgQebVV6WuXf2DzB//KO3bR5ABlRkAQDTYVWNaM01rUktpqVWNaWc+TGsNDdKgQdKuXS1tp5wibd1qhRtAojIDAOisYEcTtBXmZngvviilp/sHmf/3/6SPPiLIwB9/HQAA4fP5rFBSVWVVW+yOJgimg83w/vUv63Tr+vqWtuHDpddftyb+Am0RZgAAoWkKMCtXSs89J+3dG9l92tkM7+mnrROtW3vzTemccyJ7K6QGwgwAoGMdzYkJhWFYq5psNsOrrZWOP96/7ac/tSb5BttbD2hCmAEAtK9pTkw4Q0nB2GyGN2iQtGOH/2VbtkhDhnT+7ZAaGH0EAATn81kVmc4GGZvN8N5/36q6tA4y//mf1lsRZBAOKjMAgODKyzs3tCRJs2ZJM2b4VWTsho6oxiBShBkAQHAdrDxqV0GBNazUqhqzYYO1MqmtaIxgIXURZgAAwbWz8iiAYUhZWdL8+daZS22OJrCrxrz1lvTd70ahn0hphBkAQHCFhdZ8l6qq9ssnTUll8eKAQyL/+lfpxz/2v7xLl8CzJ4FIMQEYABCcx2MdECm1v0Y6yGnXhhEYZD78kCCD6CLMAADaV1JiBZX+/f3bs7Ot3X/XrJG2b/cLMnfeGZh9vvENq7jzzW/GvstILQwzAQA6VlJiHRBZXm5NCs7LC5gTI0mNjfZnSO7cGZiFgGghzABAqmo6nqCdcOLH45GKioI+PXCg9Pnnge2sVEKsEWYAIJW0d76S12vNj2kz76UjX30lde8e2F5VJeXnd7K/QAgIMwCQKjo6X2nnTuvYApuJvMEEmxNMNQbxxARgAEhmPp+0dq10223SpZd2vJuvaUrXXdfhcqPdu+2DzN69BBnEH2EGAJJVWZk1kaW42NqJN1T790uzZwd92jCk3NzAdtO09swD4o0wAwDJqOmk60jPVVq4MKA6s3mzfTXmX/+iGgNnEWYAINlE46TrAwesicLHGIZ01lmBl5mm/eRfIJ4IMwCQbKJx0rUkVVfrT3+yr8Z8/TXVGCQOVjMBQLLpzEnXrRgTxtu2E2KQaKjMAECyCeekaxuP6BYZCkwsjY0EGSQmKjMAkCyaNsSrqrKWFe3bF/Yt7EKM1ytVVkajg0BsUJkBgGTQehn2f/xH2EHmBj1uG2RMkyCDxEdlBgDcrmkZdoRjQHYhZswY6eWXO9kvIE4cr8zMnDlThmH4PXJb7cZkmqZmzpyp/Px8de/eXUVFRdq6dauDPQaABOHzSX//uzRpUvAgYxhSdrZ0660BO9oZMoNWYwgycBPHw4wknXbaaaqurm5+bNmypfm5Bx54QA8//LAee+wxbdy4Ubm5uTr//PN16NAhB3sMAA5rGlb64Q+tPWGCMU3rjIFLLpFqaqQ1a6Tnn7cNMTNmMMEX7pQQw0xdu3b1q8Y0MU1TCxYs0IwZM1Ry7NCzZ599Vjk5OXr++ed1/fXXx7urAOAsn886auCee8J7XXW15PHIKC6yfZoQAzdLiMrMtm3blJ+fr0GDBunKK6/UZ599Jknavn27ampqNHr06OZr09PTNXLkSK1fvz7o/err61VXV+f3AADXa6rGhBtkJDXm5Nlufjd1KkEG7ud4Zebcc8/V7373O51yyinavXu3fv3rX2vEiBHaunWrampqJEk5OTl+r8nJydHnn38e9J5z587VrFmzYtpvAIirSCf5GoYMs1EaFfgUIQbJwvHKzIUXXqhLL71Up59+un74wx/qlVdekWQNJzUx2vxzwjTNgLbWpk+frtra2uZHJesKAbiBzyetXSstXWp9bTroMcKzlr7Q8VaQaeO++wgySC6OV2ba6tmzp04//XRt27ZNY8eOlSTV1NQor9WOlnv27Amo1rSWnp6u9PT0WHcVAKKnrMwKLK3PVPJ6pYcftjbBC/OsJbsJvhIhBsnJ8cpMW/X19frggw+Ul5enQYMGKTc3V6tXr25+vqGhQevWrdOIESMc7CUARFHTEFLbwLJzp3T55dJtt4V8q/d0mm2QKSsjyCB5OV6Zuf3223XRRRdpwIAB2rNnj37961+rrq5OEydOlGEYKi0t1Zw5czR48GANHjxYc+bMUY8ePTRhwgSnuw4AnRfhEJIdqjFIVY6HmZ07d2r8+PHat2+fsrOz9b3vfU8bNmzQiSeeKEmaNm2ajhw5osmTJ+vgwYM699xztWrVKvXu3dvhngNAFJSXhz2E1Nbv9e+6Sr8PaP/gA+lb3+rUrQFXMEwz+TN7XV2dMjMzVVtbq4yMDKe7AwAtli6VOlFpphqDZBbq7++EmzMDAEmv9aql3bsjusV/6lnbIPPFFwQZpB7Hh5kAIKXYrVryeFqWYYeAagzgjzADAPESbOO7EINMhmp1SIGldp9P6kKdHSmMMAMAseDzWZN7q6ulvDxpxIiOVy21U6GhGgMER5gBgGizG0rKypL27Wv/dT6fNH++lJNjBaB9+2SMu8z2UkIM0IIwAwDRFGwoqaMg0yQnRxo/XpJsD4aUCDJAW4QZAIiWaGyAl2d/urVEiAGCIcwAQLR0ZgM8w5C8XhnFRbZPE2SA4AgzABAt1dWRvc4wrNOtKwOfIsQAHWMxHwBES15eaNdlZzd/a0pWkLFBkAFCQ2UGADqjaQl2VZW0enXH1/frZw1FrV/PkBIQJYQZAIiU3RLsjpimDh/xqHdxUcBT3/++lYsAhIcwAwCRCLYEuwPGgf3S8YHtVGOAyDFnBgDCFcES7A/1TdtdfGfPJsgAnUVlBgDCFeYSbI4iAGKLygwAhCvEJdgv6jLbIPP3VT6CDBBFVGYAIFwhLMEOWo15qUw6vyTaPQJSGpUZAAhXYaHk9do+dYnKbIPMTqNA5ovLpRKCDBBtVGYAIFwej7RwYcBqpqDVGBnSC3+wrgcQdVRmAKAtn09au1ZautT66vMFXlNSIi1fbp2nJNM2yNQrTWbBAOmll6Rx42LebSBVGaaZ/NPQ6urqlJmZqdraWmVkZDjdHQCJzG4jPK/XqsTYDBEFPeH6989J/ftbQ1IeT4w6CyS3UH9/M8wEAE1HEqxcKS1YEPh8VZU1RLS8Zc5L0BDT/M/Df49FTwHYYJgJQGorK5MGDpSKi+2DjNSSUEpLJZ8vhCADIJ6ozABIPR1VYuyYpozKCtv/ahJiAGcRZgCklkgOhxS7+AKJjDADIHVEcDgkIQZIfMyZAZAaIjgckiADuANhBkBqCONwyGD7xpgvlRFkgAREmAGQGkI4HPIrpbd/phJHEQAJiTkzAJJTQ4O0aJH06afSSSdJp53W7uVBQ8yatcc2viPIAImKMAMg+UybJj38sP8xBF26SL16SV9+6Tfp5SOdom/po4BbnDv4gDZ83FdSUez7C6BTCDMAksu0adK8eYHtjY3S4cPW94Zh7RsTrBrztU/y9I1hJwFEE3NmACSPhgarItOeLl30bOattkFmzpxjRRvOUgJchcoMgOSxaJH9CdetGI0+6YvAdlYpAe5FZQZA8vj006BPjdUK22rMG28QZAC3ozIDIHmcdJJtM5vfAcmNygwA9/H5pLVrpaVLra9NQ0uTJ/vNdwm2+d3B3Q0EGSCJEGYAuEtZmTRwoFRcLE2YYH3Ny5Nuu01av976qnaqMXdM0/EnpMWxwwBizTDN5P/3SV1dnTIzM1VbW6uMjAynuwMgUiEcFBksxDR26Srj51OlBx6IVe8ARFmov7+ZMwPAHXw+6brrIgoy5vwF0uR/SWlUZIBkRJgBkJh8PutwyKoqafdu6eWXpf37bS9tf/M7j6TSmHUTgPMIMwAST1mZNGVKSKdcBw0yMqTyNVJRUZQ7ByDREGYAJJYQ5sVIHYSYJiGclA3A/VjNBCBx+HxWRSYaQUayVjkBSHpUZgAkjrVr2x1aCjnEGIbk9UqFhVHsHIBERWUGQGIoK5Muv9z2KZ+6hBdkJGnBAg6MBFIElRkAzmlasbRypRU+bIQcYpp4vda9Skqi00cACY8wA8AZHaxYqlK+vKoKaO8in3x2/+kqLZXGjLGGlqjIACmFMAMg/jpYsRRWNcbrlRYupBIDpDDmzACIrbaHQjY0BF2x9Gf9xDbITNKT/kEmI8OqxKxZI+3YQZABUhyVGQCxYzeUlJUl7dsXcGmH1Zjx46WTTrI2wSsqYigJQDMqMwBio2koqe2cmDZB5jo9YRtkynSJfzXmmmuk++6TRo0iyADwQ2UGQPRFe/M7SdqzJxo9A5CEqMwAiL7y8g43v7MLMp9rQPAl1+zmCyAIKjMAoq+dM5HC3jeG3XwBdIDKDIDos6miBKvGHFXX9oOMxG6+ANpFmAHQeQ0N0kMPSZdcIl11lXTkiFVNORZG2qvGdJWvpaFfP/8LvF5p+XKWXgNoF8NMADpn2jQryDQ2trT9/vdSt24yzEbbl9hWYvr1k3btktavt4ap8vLYzRdASAgzACI3bZo0b57tU8ZXR2zbgw4p3XqrlJZm7SEDAGEwTLODtZNJoK6uTpmZmaqtrVVGRobT3QGSQ0OD1KOHtQy7lbAn+EpWVWb3bqowAPyE+vubOTMAIrNoUehBZta91vwZI0igefJJggyAiBFmAITO55P+/ndpxgzp8cebm4OtVDKPPaPBg62JvP37+19QUCC99BITfAF0CnNmALTP57M2wVu5Uvrv/5bq6pqfMiV1CWVYKS/PmgszZox1Lyb4AogiwgyA4OwOijwm5Lkx+fktG955PEzwBRB1hBkA9poOimyzRqBWGTpetbYvsZ3ke/31VF8AxJRr5swsWrRIgwYNUrdu3TR06FCVl5c73SUgeQU5KNKQaRtkmufG2Bk8OBY9BIBmrggzL7zwgkpLSzVjxgy98847Kiws1IUXXqiKigqnuwYkpzYHRb6pc2yHlb6rt9pfci1xQCSAmHPFPjPnnnuuzj77bD3eavXEt7/9bY0dO1Zz587t8PXsMwOEaelSacIESRHuGyO1HBC5fTvDTAAikjT7zDQ0NGjTpk0aPXq0X/vo0aO1fv1629fU19errq7O7wEgDNu2aYGm2AaZ32pyaEFG4oBIAHGR8BOA9+3bJ5/Pp5ycHL/2nJwc1dTU2L5m7ty5mjVrVjy6BySfsjIZ99xt+1SHIaaJ12sFGfaPARAHCV+ZaWK02TnUNM2AtibTp09XbW1t86OysjIeXQRc7/vnmTIuDQwg7+m00IJMaam0Zo01tESQARAnCV+ZycrKksfjCajC7NmzJ6Ba0yQ9PV3p6enx6B6QNKx/GwQGlpBCjNcrLVxIgAHgiISvzKSlpWno0KFavXq1X/vq1as1YsQIh3oFJI9gRyb9S907DjJNlZgdOwgyAByT8JUZSZo6daquuuoqDRs2TMOHD9eTTz6piooK3XDDDU53DXC1YOc+dhhisrOlxYsJMAASgivCzBVXXKH9+/fr3nvvVXV1tYYMGaK//OUvOvHEE53uGuAOTecrHTsTySgusr0spCGl7GxrD5q0tGj2EAAi5op9ZjqLfWaQ0pYvlyZPlvbuldTJfWOa7kdFBkAcJM0+MwA6Ydo0adw4ae/eYwcOBAaZdo8iaM3rJcgASEiuGGYCEIEXX5TmzZPUiWqMJP3yl9KoUdbJ12yAByABEWaAZOTzSZMndy7ENDn1VKmoKDr9AoAYYJgJSEbl5TL27bV9KqwgI3FQJICER2UGSCY+n4yuHklFAU+FHWKaDoosLIxK1wAgVqjMAEmi4YUVx4JMoKBBpml1QNsNZzgoEoCLEGYAt/P5ZBhS+pWXBDzV4Uqlp5+WXnpJ6t/fv52VSwBchGEmwG1abYC3/a29+saCWwMuGaIt2qIz2r/PHXdIl11mfT9mjN+meqxcAuAmhBnATcrKpClTpJ07I1+plJ0t/fa31v4zTTweViwBcC2GmQC3KCuTLrtML+wcYRtk7te09oPMzTdbh0JWV/sHGQBwOSozgBs0NEjXXy/DbLR9OqSVSpdeSvUFQFKiMgMkurIyXZPxou2+MW/oe6GdqVRQwBJrAEmLygyQqHw+afZsGffcbft0WPvGsMQaQBKjMgMkorIyde1q2gaZ/eobepBhiTWAFEBlBkg0L74o43L7CbphVWNmzZJmzKAiAyDpEWaABGJtvBsYZBplhHcYQWmpdLf98BQAJBvCDJAg2p4o0CTsM5UkaxM8AEgRhBnAYVENMRwOCSAFMQEYcMqxM5XsRBxkJFYuAUg5hBnAAYYh2xOuOzwYsj2sXAKQohhmAuIsKtUYr1d6+GHrnCUOhwSQ4ggzQJx0OsQYhnTLLdIllxBcAKAVwgwQY42NwXNHWNWYZcukyy+PTqcAIIkQZoAYitoE3zvuIMgAQBBMAAZi4OBB+yDzHb0TXpDJzJT+8AfpgQei1zkASDJUZoAoi1o1Jjtb2rlTSkvrfKcAIIlRmQGiZMsW+yAzV3dGttx68WKCDACEgMoMEAVBqzHZJ0h794Z3s379pCefZL8YAAgRlRmgE5YutQ8y6/RvVjUmnCDTp4910vXu3QQZAAgDlRkgQlE9imDWLGnGDPaOAYAIUJkBwjR5sn2Q2aPsyIPM3XcTZAAgQlRmgDBEtRojWccSzJgReYcAAFRmgFAUFNgHma/liTzIGIa0cCEVGQDoJMIM0AHDsLZ7actcs1YeNUZ20+xsTrgGgChhmAkIIuiQ0tc+q5riK5R695YOHQrvxmyGBwBRRWUGsNHu3JiBA6WyMivQXHBBeDc1DDbDA4AoI8wArTTljbZMGS1zY3bulC691Ao0kyaFfnOvl6ElAIgBwgxwTNgrla67LvSbP/igtH07QQYAYoA5M0h5ES+33r9fWrcutDfJz2fVEgDECJUZpDS7IFOgitCXW1dWhnZdXl7onQIAhIUwg5QUdG5M336q0Imh36igwJoLE6y8YxjWNYWFkXUUANAhwgxSSn29fe4YN87aN0YHDoR3wx/8wNr4Tgq8cdPPCxYwxAQAMcScGaSMoHNjzGPfPFcV3g379ZOKiqygsny5NGWK/+56Xq8VZJj0CwAxRZhB0quqsnJFW48+Kt18o09aWy6tXCk980x4N37yyZaKS0mJNGaMVF4uVVdbc2QKC6nIAEAcEGaQ1NqtxpSVSQOn2J9V0J7+/aVHHgmsuHg8VqUGABBXzJlBUnrtNfsg89prrYLMZZeFH2RmzZI+/5yhIwBIIFRmkHQ6nBvj81nzW5obQpCdbR1DQIgBgIRDZQZJ47/+yz7IVFW1yS3l5eFXZObPJ8gAQIKiMoOk0GE1prXq6vDfoH//8F8DAIgLKjNwtVtvtQ8y9fXtjCKFsxsvm94BQMKjMgPXCqsa01phobVWO2D8KcgbsOkdACQ0KjNwnTPOCHIUgRninF6PJ/iuva15vdZmeMyVAYCERpiBqxiGtGVLYHs4C5MkWQFl+fLAuTDZ2VJpqbRmjbR9O0EGAFyAYSa4QlhDSj5faDvxsmsvACQFwgwSXkj7xqxdaz0+/ND6um9fy4VerzWsZFdlYddeAHA9wgwSVkjVmLIy6brrpP37g9+oqsra7Zf5LwCQlJgzg4RkF2ROO80myFx6aftBRmp5UWmpVcUBACQVwgwSimEEX6n03nutGnw+a5OZUJmmVFlpzY8BACQVwgwSQmOjfYi58cYgk3zLy63ho3BFsvsvACChMWcGjgtpgm/bFUeRhpJwdv8FALgClRk45vBh+yDz1FOtgkxZmTRwoFRcLE2YYH0dOFDati28N+NYAgBIWlRm4IiQVypddlngOFNVlTRzptS3r3TgQOhvyrEEAJCUqMwgrnbtsg8yb73VJrP4fNKUKfYTZpra2juKoLWCApZlA0ASozKDuAlrF9/ycmnnzuA3M01rSfasWdIjjwQuz87IkH72M2uHX3b1BYCkRphBzL3zjnT22YHtNTVSTk6QF4U6wXfwYGn37pYdgCVrR9+iIgIMAKQIwgxiKuRqTNsVSyecENob5OVZoWXUKOsBAEg5js6ZGThwoAzD8HvceeedftdUVFTooosuUs+ePZWVlaVbb71VDQ0NDvUYoVq50j7IfPWVTZCxW7E0caLUr1/wNMTqJADAMY5XZu69915NmjSp+edevXo1f+/z+fSTn/xE2dnZev3117V//35NnDhRpmnq0UcfdaK7CEG71RifT1rbqgKzd690xRWBCWfXLv+Jvq2fb3oDVicBAJQAYaZ3797Kzc21fW7VqlV6//33VVlZqfz8fEnSQw89pKuvvlqzZ89WRkZGPLuKDjzwgPSLXwS2N+/uW1ZmrVBqPbG3S5fgK5YMw1p+3b27/2u8XivIsDoJAKAECDP333+/7rvvPhUUFGjcuHG64447lJaWJkl64403NGTIkOYgI0kXXHCB6uvrtWnTJhUXF9ves76+XvX19c0/19XVxfYPgY7nxgTbM6axMfhNm1Ys/e1vVgWm9Q7AVGQAAMc4GmamTJmis88+W3369NFbb72l6dOna/v27XrqqackSTU1Ncpps9ylT58+SktLU01NTdD7zp07V7NmzYpp32G5/XbpoYcC20PeMyYUe/ZI48dH9loAQNKL+gTgmTNnBkzqbft4++23JUm33XabRo4cqTPOOEPXXnutFi9erKefflr7W+0ZYtj8k980Tdv2JtOnT1dtbW3zo7KyMtp/TMiqxrQNMiedZJNZOtozpiOcpwQAaEfUKzM333yzrrzyynavGThwoG379773PUnSJ598on79+ik3N1dvvvmm3zUHDx7U0aNHAyo2raWnpys9PT28jiNkF1wgrVoV2B608BLpoZCGYc2PYcUSAKAdUQ8zWVlZysrKiui177zzjiQp79i/xIcPH67Zs2erurq6uW3VqlVKT0/X0KFDo9NhhMWuIDZ1qv1QU7POVFZYsQQA6IBjc2beeOMNbdiwQcXFxcrMzNTGjRt122236eKLL9aAAQMkSaNHj9app56qq666SvPmzdOBAwd0++23a9KkSaxkirO+faWDBwPbQ5oGU1hoVViqqkKfN5OdLS1ezIolAECHHNs0Lz09XS+88IKKiop06qmn6u6779akSZO0dOnS5ms8Ho9eeeUVdevWTeedd54uv/xyjR07Vg8++KBT3U45TSuk2waZxYvDmM/r8UgLF1rfh3I4ZHa2NceGIAMACIFhmpEuMXGPuro6ZWZmqra2lopOGHr0kI4cCWyP+G+M3T4zrTUFHU64BgAo9N/fjh5ngMR09KiVK9oGmX/8I0iQ8fmsQx6XLrW++nz2Ny4pkXbskNaskUpLpbZzq7xeggwAIGxUZuAn5IMhm9hVW7xea1ipo1DS9nBJNsMDALQS6u9vwgwkSV9+KbU6FqvZtm3SyScHeVGwXX0ZLgIARAHDTAiZYdgHGdNsJ8i0t6tvU1tpafAhJwAAooQwk8J277YfVtq3L4RJvh3t6muaUmWldR0AADHk+EGTcEbYc2PaCnVX30h3/wUAIESEmRSzb5+1jUtb9fXSscPKA9lN1A11V1/OVQIAxBjDTClkxIjAIPPtb1vVmKBBpqxMGjhQKi6WJkywvg4cKO3da61aClbiMQypoIBzlQAAMUeYSQEVFVa2eOMN//bGRun999t5YdNqpbZzY6qqpCuukMaPt35uG2iafuZcJQBAHBBmklz//tKJJ/q3LVvWckxBUKGsVlq2TPrDH6w3aY3N7wAAccScmSS1d690wgmB7SFP8A11tVJWlrWrL5vfAQAcQmUmCf3qV4FBZtWqMM9UCme1kscjFRVZw05FRQQZAEBcUZlJIpWV0oAB/m233tpyYHW72q5Ysivr2GG1EgDAYYSZJHHLLdJjj/m37dwZOJ3Fdpn1ypX25yv16ycdOGBf0jEM6xpWKwEAHEaYcblt26RTTvFvu+ceaeZMm4vtDoXs10/avz/w2qqqlhBjGP6BhtVKAIAEQphxsQkTpKVL/dv27rXm5AYIdiikXZCRWpY79e0rdetmhZsmXq8VZFitBABIAIQZF3r3XenMM/3b5s+3znW01d4y6/aYphV2/vY3qwLDaiUAQAIizLiIaUo/+pG1Mqm12lqpnZPRO15m3ZE9e1o2yAMAIMGwNNsl3nhD6tLFP8j8939bAafdICN1/rBHViwBABIYlZkE19gonXOOtGlTS1vPntbcmO7dQ7xJpGGEFUsAABegMpPAmqaqtA4yy5dLhw+HEWQkK4zYzgpuByuWAAAuQWUmAR09Kn3zm9L27S1tBQXSJ5+0c7q1ndZ7yhQVWUkoVKxYAgC4BGEmwaxYEZgfXn1VuuCCMG9kt6dMKH75S2nUKFYsAQBcgzCTII4ckXJypEOHWtrOOkvauDGCTBFsT5n2NM2PmTmTEAMAcBXmzCSAZ5+VevTwDzL/+If0z39GkCsi2VOG+TEAABejMuOgQ4cCl1WPGiWtXt2SL8IWyZ4yzI8BALgYYcYhjz5qnWjd2jvvSN/5TidvHOqeMr/8pXTqqezoCwBwPcJMnO3fH7hK+oorpGXLovQGoe4pM2qUtcIJAACXY85MHN13X2CQ+eijKAYZyaqyeL3Bx6kMw1rnzUZ4AIAkQZiJg127rAxx990tbTfcYM3RPeWUKL+ZxyMtXGh93zbQMNEXAJCECDMxNnWq1L+/f9vnn0uPPx7DNy0psTbIa/vGXq/VzkRfAEASYc5MjHz2mXTSSf5t06dLc+bEqQMlJdKYMS07ADPRFwCQpAgzMXD11dbeMa3t3i2dcEKcO+LxMMkXAJD0GGaKovfft6altA4y999vzY2Je5ABACBFUJmJAtO0RnT+9Cf/9oMHpeOPd6RLAACkDCoznbRxo9Sli3+QeeIJK+AQZAAAiD0qM53wz39K55zT8vNxx1nVmJ49nesTAACphspMJ7z7bsv3S5dKDQ0EGQAA4o3KTCdMnCidd540cKBVlQEAAPFHmOkEw5AGD3a6FwAApDaGmQAAgKsRZgAAgKsRZgAAgKsRZgAAgKsRZgAAgKsRZgAAgKsRZgAAgKsRZgAAgKsRZgAAgKuxA3Bn+HxSeblUXS3l5UmFhZLH43SvAABIKYSZSJWVSVOmSDt3trR5vdLChVJJiXP9AgAgxTDMFImyMumyy/yDjCRVVVntZWXO9AsAgBREmAmXz2dVZEwz8LmmttJS6zoAABBzhJlwlZcHVmRaM02pstK6DgAAxBxhJlzV1dG9DgAAdAphJlx5edG9DgAAdAphJlyFhdaqJcOwf94wpIIC6zoAABBzhJlweTzW8mspMNA0/bxgAfvNAAAQJ4SZSJSUSMuXS/37+7d7vVY7+8wAABA3bJoXqZISacwYdgAGAMBhhJnO8HikoiKnewEAQEpjmAkAALgaYQYAALgaYQYAALgaYQYAALhaTMPM7NmzNWLECPXo0UPHH3+87TUVFRW66KKL1LNnT2VlZenWW29VQ0OD3zVbtmzRyJEj1b17d/Xv31/33nuvTLuDHgEAQMqJ6WqmhoYGjRs3TsOHD9fTTz8d8LzP59NPfvITZWdn6/XXX9f+/fs1ceJEmaapRx99VJJUV1en888/X8XFxdq4caM+/vhjXX311erZs6d+/vOfx7L7AADABWIaZmbNmiVJWrJkie3zq1at0vvvv6/Kykrl5+dLkh566CFdffXVmj17tjIyMvTcc8/pq6++0pIlS5Senq4hQ4bo448/1sMPP6ypU6fKCHasAAAASAmOzpl54403NGTIkOYgI0kXXHCB6uvrtWnTpuZrRo4cqfT0dL9rdu3apR07dtjet76+XnV1dX4PAACQnBwNMzU1NcrJyfFr69Onj9LS0lRTUxP0mqafm65pa+7cucrMzGx+FBQUxKD3AAAgEYQ9zDRz5szm4aNgNm7cqGHDhoV0P7thItM0/drbXtM0+TfYENP06dM1derU5p9ra2s1YMAAKjQAALhI0+/tjhb9hB1mbr75Zl155ZXtXjNw4MCQ7pWbm6s333zTr+3gwYM6evRoc/UlNzc3oAKzZ88eSQqo2DRJT0/3G5Zq+jCo0AAA4D6HDh1SZmZm0OfDDjNZWVnKysrqVKeaDB8+XLNnz1Z1dbXy8vIkWZOC09PTNXTo0OZr7rrrLjU0NCgtLa35mvz8/JBDU35+viorK9W7d++kmDBcV1engoICVVZWKiMjw+nuuAafW+T47CLD5xY5PrvIJdNnZ5qmDh065De31k5MVzNVVFTowIEDqqiokM/n0+bNmyVJJ598snr16qXRo0fr1FNP1VVXXaV58+bpwIEDuv322zVp0qTm/wEmTJigWbNm6eqrr9Zdd92lbdu2ac6cObr77rtDDiZdunSR1+uN1R/TMRkZGa7/i+oEPrfI8dlFhs8tcnx2kUuWz669ikyTmIaZu+++W88++2zzz2eddZYkac2aNSoqKpLH49Err7yiyZMn67zzzlP37t01YcIEPfjgg82vyczM1OrVq3XTTTdp2LBh6tOnj6ZOneo3JwYAAKQuw2QrXdepq6tTZmamamtrkyJ1xwufW+T47CLD5xY5PrvIpeJnx9lMLpSenq577rnHb5IzOsbnFjk+u8jwuUWOzy5yqfjZUZkBAACuRmUGAAC4GmEGAAC4GmEGAAC4GmEGAAC4GmHGxXbs2KFrrrlGgwYNUvfu3XXSSSfpnnvuUUNDg9NdS3izZ8/WiBEj1KNHDx1//PFOdyehLVq0SIMGDVK3bt00dOhQlZeXO90lV3jttdd00UUXKT8/X4Zh6OWXX3a6S64wd+5cffe731Xv3r11wgknaOzYsfroo4+c7lbCe/zxx3XGGWc0b5Q3fPhw/fWvf3W6W3FDmHGxDz/8UI2NjXriiSe0detWzZ8/X4sXL9Zdd93ldNcSXkNDg8aNG6cbb7zR6a4ktBdeeEGlpaWaMWOG3nnnHRUWFurCCy9URUWF011LeF9++aXOPPNMPfbYY053xVXWrVunm266SRs2bNDq1av19ddfa/To0fryyy+d7lpC83q9+s1vfqO3335bb7/9tn7wgx9ozJgx2rp1q9NdiwuWZieZefPm6fHHH9dnn33mdFdcYcmSJSotLdUXX3zhdFcS0rnnnquzzz5bjz/+eHPbt7/9bY0dO1Zz5851sGfuYhiGVqxYobFjxzrdFdfZu3evTjjhBK1bt07/9m//5nR3XKVv376aN2+errnmGqe7EnNUZpJMbW2t+vbt63Q3kAQaGhq0adMmjR492q999OjRWr9+vUO9Qqqpra2VJP67Fgafz6dly5bpyy+/1PDhw53uTlzE9GwmxNenn36qRx99VA899JDTXUES2Ldvn3w+n3Jycvzac3JyVFNT41CvkEpM09TUqVP1/e9/X0OGDHG6Owlvy5YtGj58uL766iv16tVLK1as0Kmnnup0t+KCykwCmjlzpgzDaPfx9ttv+71m165d+tGPfqRx48bp2muvdajnzorkc0PH2p5Ob5pmyCfWA51x8803691339XSpUud7oorfPOb39TmzZu1YcMG3XjjjZo4caLef/99p7sVF1RmEtDNN9+sK6+8st1rBg4c2Pz9rl27VFxcrOHDh+vJJ5+Mce8SV7ifG9qXlZUlj8cTUIXZs2dPQLUGiLZbbrlFf/zjH/Xaa6/J6/U63R1XSEtL08knnyxJGjZsmDZu3KiFCxfqiSeecLhnsUeYSUBZWVnKysoK6dqqqioVFxdr6NCheuaZZ9SlS+oW28L53NCxtLQ0DR06VKtXr9Yll1zS3L569WqNGTPGwZ4hmZmmqVtuuUUrVqzQ2rVrNWjQIKe75Fqmaaq+vt7pbsQFYcbFdu3apaKiIg0YMEAPPvig9u7d2/xcbm6ugz1LfBUVFTpw4IAqKirk8/m0efNmSdLJJ5+sXr16Odu5BDJ16lRdddVVGjZsWHPlr6KiQjfccIPTXUt4hw8f1ieffNL88/bt27V582b17dtXAwYMcLBnie2mm27S888/r5UrV6p3797NlcHMzEx1797d4d4lrrvuuksXXnihCgoKdOjQIS1btkxr167Vq6++6nTX4sOEaz3zzDOmJNsH2jdx4kTbz23NmjVOdy3h/Pa3vzVPPPFEMy0tzTz77LPNdevWOd0lV1izZo3t37GJEyc63bWEFuy/ac8884zTXUtoP/vZz5r/f5qdnW2OGjXKXLVqldPdihv2mQEAAK6WuhMsAABAUiDMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAVyPMAAAAV/v/1YEDp5rEdMsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise = 1, random_state=32 )\n",
    "\n",
    "x = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1. model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for num in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(y, y_pred)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (num+1) % 10 == 0:\n",
    "        print(f'epoch: {num+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "\n",
    "predicted = model(x).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.5765\n",
      "epoch: 20, loss = 0.4885\n",
      "epoch: 30, loss = 0.4290\n",
      "epoch: 40, loss = 0.3859\n",
      "epoch: 50, loss = 0.3531\n",
      "epoch: 60, loss = 0.3271\n",
      "epoch: 70, loss = 0.3060\n",
      "epoch: 80, loss = 0.2885\n",
      "epoch: 90, loss = 0.2736\n",
      "epoch: 100, loss = 0.2609\n",
      "accuracy = 0.9123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1. model\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        predicted = torch.sigmoid(self.linear(x))\n",
    "        return predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #updates\n",
    "    optimizer.step()\n",
    "    #zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset \n",
    "\n",
    "class TitanicData(Dataset):\n",
    "    def __init__(self, pre_process=None):\n",
    "        super(TitanicData, self).__init__()\n",
    "        data = pd.read_csv('./data/titanic.csv')\n",
    "        if pre_process:\n",
    "            self.features = PreProcess()\n",
    "\n",
    "        #self.features = data.drop('Survived', axis=1).to_numpy()\n",
    "        self.target = data[['Survived']].to_numpy()\n",
    "        \n",
    "        self.target_col = 'Survived'\n",
    "        self.feature_cols = [col for col in data.columns if col != self.target_col]\n",
    "        self.pre_process = pre_process\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.features[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.features[index], self.target[index]\n",
    "        if self.pre_process:\n",
    "            sample = self.pre_process(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class PreProcess:\n",
    "    def __call__(self, df):\n",
    "        return df.iloc[:,[0,1,5,6,7]]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = TitanicData(pre_process=PreProcess())\n",
    "\n",
    "#data_loader = DataLoader(dataset=my_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# total_samples = len(my_data)\n",
    "# n_iterations = total_sample/batch_size\n",
    "#for epoch in range(num_epochs):\n",
    "#   for i, (inputs, labels) in enumerate(data_loader):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PreProcess at 0x1ea80063640>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "#dataset = torchvision.datasets.MNIST(root='MNIST/raw/train-images-idx3-ubyte' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions \n",
    "* Apply a non-linear transformation and decide whether a neuron should be activated or not.\n",
    "\n",
    "1. Step function \n",
    "2. Sigmoid \n",
    "3. Tanh\n",
    "4. RelU\n",
    "5. Leaky ReLU -- modified version of relu to update the weights better\n",
    "6. Softmax -- Good for multi class classification problems in last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task or in other words it allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
    "\n",
    "For Eg:\n",
    "- we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
    "\n",
    "- Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
    "\n",
    "*******************************\n",
    "\n",
    "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
    "\n",
    "And there are several places you can find pretrained models to use for your own problems.\n",
    "\n",
    "\n",
    "\n",
    "| Location | what's there ? | Link(s)|\n",
    "|----|----|-----|\n",
    "|PyTorch domain libraries | Each of the PyTorch domain libraries (torchvision, torchtext) come with pretrained models of some form. The models there work right within PyTorch. | torchvision.models, torchtext.models, torchaudio.models, torchrec.models\n",
    "|HuggingFace Hub |A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. |\thttps://huggingface.co/models, https://huggingface.co/datasets\n",
    "|timm (PyTorch Image Models) library |\tAlmost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. |\thttps://github.com/rwightman/pytorch-image-models\n",
    "|Paperswithcode |\tA collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. \t|https://paperswithcode.com/|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([i for i in range(1,10,2)])\n",
    "t2 = torch.tensor([i for i in range(1,20,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5,  9, 13, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  8, 14, 20, 26])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  15,  45,  91, 153])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1*t2  # elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1,2,5,6,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([146])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.matmul(t2.T) # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.5000, 1.0000, 1.1667, 1.1250]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  7,  9, 11, 13])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add_(2*2)  # If tensor stored in CPU, inplace operations to a tensor will share common memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7,  9, 11, 13], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # checks if gpu available \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    z = x **3\n",
    "    z = z.to(device) # moves to gpu\n",
    "\n",
    "    y = x+z # runs on gpu\n",
    "\n",
    "    y.numpy() ## ERROR, numpy can only handle CPU tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of m : 3.0\n",
      "Grad of c : 1.0\n"
     ]
    }
   ],
   "source": [
    "# regression slope form\n",
    "# y = mx + c\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "\n",
    "m = torch.tensor(1.0, requires_grad=True)\n",
    "c = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "y = m*x + c\n",
    "\n",
    "y.backward() # dy/dx = m + 1 \n",
    "print(f'Grad of m : {m.grad}') # --> 3\n",
    "print(f'Grad of c : {c.grad}') # --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1850,  0.2318,  0.2517,  2.4815, -0.9783], requires_grad=True)\n",
      "tensor([ 0.1850,  0.2318,  0.2517,  2.4815, -0.9783])\n",
      "tensor([ 0.1850,  0.2318,  0.2517,  2.4815, -0.9783])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "z = x.requires_grad_(False) ## setting the grad to tensor\n",
    "print(z)\n",
    "\n",
    "y = x.detach() ## new tensor detaching the grad functn\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1145,  5.0146,  8.0146, 10.0146, 12.0146])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([.1,2,5,7,9], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x + 2.03*1.485\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(5):  # Training loop\n",
    "    model_output = (weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_() # setting the grad zero for every loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```x ---> a(x) ---->y b(y) ----> z```\n",
    "\n",
    "**Chain Rule:**\n",
    "Gradient -> dz/dx = dz/dy*dy/dx \n",
    "\n",
    "```Computational Graph```\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* 3 steps process:\n",
    "1. Forward pass: Compute loss\n",
    "2. Compute local gradients\n",
    "3. Backward pass: Compute ```d(Loss)/d(Weights)``` using chain rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "## Example \n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass & calc loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can build any model by 4 ways:\n",
    "\n",
    "* case: 1\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Manually\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 2\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 3\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "\n",
    "* case: 4\n",
    "    * Prediction: PyTorch Model\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 9.99999977350235\n"
     ]
    }
   ],
   "source": [
    "# f = w*x\n",
    "# f = 2*x\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "## Gradient \n",
    "## MSE          =    1/N *  (w*x - y)**2\n",
    "## d(MSE)/dw    =    1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    dw = gradient(X,Y,y_pred) # gradients\n",
    "\n",
    "    w -= learning_rate * dw  # update weights \n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=0.01)\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "prediction before training: f(5) = -0.647\n",
      "epoch 1: w = 0.275, loss = 35.70541763\n",
      "epoch 11: w = 1.337, loss = 1.55933607\n",
      "epoch 21: w = 0.780, loss = 3.91086841\n",
      "epoch 31: w = 0.705, loss = 2.22391820\n",
      "epoch 41: w = 1.077, loss = 1.30657768\n",
      "epoch 51: w = 1.444, loss = 0.51164573\n",
      "epoch 61: w = 1.639, loss = 0.17969069\n",
      "epoch 71: w = 1.723, loss = 0.10028634\n",
      "epoch 81: w = 1.780, loss = 0.06345269\n",
      "epoch 91: w = 1.835, loss = 0.03662919\n",
      "prediction after training: f(5) = 9.673\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(f'prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = model(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 1081.1012\n",
      "epoch: 20, loss = 703.1006\n",
      "epoch: 30, loss = 457.6850\n",
      "epoch: 40, loss = 298.2519\n",
      "epoch: 50, loss = 194.6111\n",
      "epoch: 60, loss = 127.1948\n",
      "epoch: 70, loss = 83.3125\n",
      "epoch: 80, loss = 54.7292\n",
      "epoch: 90, loss = 36.0980\n",
      "epoch: 100, loss = 23.9451\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise = 1, random_state=32 )\n",
    "\n",
    "x = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1. model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for num in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(y, y_pred)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (num+1) % 10 == 0:\n",
    "        print(f'epoch: {num+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "\n",
    "predicted = model(x).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.7877\n",
      "epoch: 20, loss = 0.5925\n",
      "epoch: 30, loss = 0.4832\n",
      "epoch: 40, loss = 0.4153\n",
      "epoch: 50, loss = 0.3691\n",
      "epoch: 60, loss = 0.3356\n",
      "epoch: 70, loss = 0.3101\n",
      "epoch: 80, loss = 0.2898\n",
      "epoch: 90, loss = 0.2734\n",
      "epoch: 100, loss = 0.2597\n",
      "accuracy = 0.9123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1. model\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        predicted = torch.sigmoid(self.linear(x))\n",
    "        return predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #updates\n",
    "    optimizer.step()\n",
    "    #zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset \n",
    "\n",
    "class TitanicData(Dataset):\n",
    "    def __init__(self, pre_process=None):\n",
    "        super(TitanicData, self).__init__()\n",
    "        data = pd.read_csv('./data/titanic.csv')\n",
    "        if pre_process:\n",
    "            self.features = PreProcess()\n",
    "\n",
    "        #self.features = data.drop('Survived', axis=1).to_numpy()\n",
    "        self.target = data[['Survived']].to_numpy()\n",
    "        \n",
    "        self.target_col = 'Survived'\n",
    "        self.feature_cols = [col for col in data.columns if col != self.target_col]\n",
    "        self.pre_process = pre_process\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.features[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.features[index], self.target[index]\n",
    "        if self.pre_process:\n",
    "            sample = self.pre_process(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class PreProcess:\n",
    "    def __call__(self, df):\n",
    "        return df.iloc[:,[0,1,5,6,7]]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = TitanicData(pre_process=PreProcess())\n",
    "\n",
    "#data_loader = DataLoader(dataset=my_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# total_samples = len(my_data)\n",
    "# n_iterations = total_sample/batch_size\n",
    "#for epoch in range(num_epochs):\n",
    "#   for i, (inputs, labels) in enumerate(data_loader):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PreProcess at 0x1ca462adee0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "#dataset = torchvision.datasets.MNIST(root='MNIST/raw/train-images-idx3-ubyte' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions \n",
    "* Apply a non-linear transformation and decide whether a neuron should be activated or not.\n",
    "\n",
    "1. Step function \n",
    "2. Sigmoid \n",
    "3. Tanh\n",
    "4. RelU\n",
    "5. Leaky ReLU -- modified version of relu to update the weights better\n",
    "6. Softmax -- Good for multi class classification problems in last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

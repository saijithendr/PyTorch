{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([i for i in range(1,10,2)])\n",
    "t2 = torch.tensor([i for i in range(1,20,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5,  9, 13, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  8, 14, 20, 26])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  15,  45,  91, 153])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1*t2  # elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[1,2,5,6,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([146])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.matmul(t2.T) # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.5000, 1.0000, 1.1667, 1.1250]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  7,  9, 11, 13])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add_(2*2)  # If tensor stored in CPU, inplace operations to a tensor will share common memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7,  9, 11, 13], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # checks if gpu available \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    z = x **3\n",
    "    z = z.to(device) # moves to gpu\n",
    "\n",
    "    y = x+z # runs on gpu\n",
    "\n",
    "    y.numpy() ## ERROR, numpy can only handle CPU tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of m : 3.0\n",
      "Grad of c : 1.0\n"
     ]
    }
   ],
   "source": [
    "# regression slope form\n",
    "# y = mx + c\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "\n",
    "m = torch.tensor(1.0, requires_grad=True)\n",
    "c = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "y = m*x + c\n",
    "\n",
    "y.backward() # dy/dx = m + 1 \n",
    "print(f'Grad of m : {m.grad}') # --> 3\n",
    "print(f'Grad of c : {c.grad}') # --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0468, -1.6157, -1.3622, -0.3899, -0.7375], requires_grad=True)\n",
      "tensor([ 1.0468, -1.6157, -1.3622, -0.3899, -0.7375])\n",
      "tensor([ 1.0468, -1.6157, -1.3622, -0.3899, -0.7375])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "z = x.requires_grad_(False) ## setting the grad to tensor\n",
    "print(z)\n",
    "\n",
    "y = x.detach() ## new tensor detaching the grad functn\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1145,  5.0146,  8.0146, 10.0146, 12.0146])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([.1,2,5,7,9], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x + 2.03*1.485\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(5):  # Training loop\n",
    "    model_output = (weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_() # setting the grad zero for every loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```x ---> a(x) ---->y b(y) ----> z```\n",
    "\n",
    "**Chain Rule:**\n",
    "Gradient -> dz/dx = dz/dy*dy/dx \n",
    "\n",
    "```Computational Graph```\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* 3 steps process:\n",
    "1. Forward pass: Compute loss\n",
    "2. Compute local gradients\n",
    "3. Backward pass: Compute ```d(Loss)/d(Weights)``` using chain rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "## Example \n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass & calc loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can build any model by 4 ways:\n",
    "\n",
    "* case: 1\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Manually\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 2\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: Manually\n",
    "    * Parameter updates: Manually \n",
    "\n",
    "* case: 3\n",
    "    * Prediction: Manually\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "\n",
    "* case: 4\n",
    "    * Prediction: PyTorch Model\n",
    "    * Gradients Computation: Autograd\n",
    "    * Loss Computation: PyTorch Loss\n",
    "    * Parameter updates: PyTorch Optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 9.99999977350235\n"
     ]
    }
   ],
   "source": [
    "# f = w*x\n",
    "# f = 2*x\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "## Gradient \n",
    "## MSE          =    1/N *  (w*x - y)**2\n",
    "## d(MSE)/dw    =    1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    dw = gradient(X,Y,y_pred) # gradients\n",
    "\n",
    "    w -= learning_rate * dw  # update weights \n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# Training\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=0.01)\n",
    "\n",
    "print(f'prediction before training: f(5) = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "prediction before training: f(5) = 4.994\n",
      "epoch 1: w = 1.046, loss = 6.64694309\n",
      "epoch 11: w = 1.518, loss = 0.50624496\n",
      "epoch 21: w = 1.325, loss = 0.91638166\n",
      "epoch 31: w = 1.341, loss = 0.56927085\n",
      "epoch 41: w = 1.534, loss = 0.32707059\n",
      "epoch 51: w = 1.711, loss = 0.13135080\n",
      "epoch 61: w = 1.806, loss = 0.05103639\n",
      "epoch 71: w = 1.853, loss = 0.02854154\n",
      "epoch 81: w = 1.885, loss = 0.01750698\n",
      "epoch 91: w = 1.914, loss = 0.00994167\n",
      "prediction after training: f(5) = 9.830\n"
     ]
    }
   ],
   "source": [
    "## Training pipeline in pytorch\n",
    "# 1. design model \n",
    "# 2. construct loss and optimizer\n",
    "# 3. training loop\n",
    "#      - forward pass: compute prediction\n",
    "#      - backward pass: gradients\n",
    "#      - update weights\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "n_iters = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "print(f'prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = model(X) # forward pass\n",
    "        \n",
    "    l = loss(Y, y_pred) #loss\n",
    "    \n",
    "    l.backward() # gradient = dl/dw\n",
    "\n",
    "    optimizer.step()  # update weights \n",
    "\n",
    "    w.grad.zero_() \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') \n",
    "\n",
    "print(f'prediction after training: f(5) = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 1057.2358\n",
      "epoch: 20, loss = 688.1647\n",
      "epoch: 30, loss = 448.3559\n",
      "epoch: 40, loss = 292.4375\n",
      "epoch: 50, loss = 190.9962\n",
      "epoch: 60, loss = 124.9536\n",
      "epoch: 70, loss = 81.9271\n",
      "epoch: 80, loss = 53.8759\n",
      "epoch: 90, loss = 35.5746\n",
      "epoch: 100, loss = 23.6255\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise = 1, random_state=32 )\n",
    "\n",
    "x = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1. model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for num in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(y, y_pred)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (num+1) % 10 == 0:\n",
    "        print(f'epoch: {num+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# plot\n",
    "\n",
    "predicted = model(x).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.5155\n",
      "epoch: 20, loss = 0.4363\n",
      "epoch: 30, loss = 0.3835\n",
      "epoch: 40, loss = 0.3459\n",
      "epoch: 50, loss = 0.3177\n",
      "epoch: 60, loss = 0.2957\n",
      "epoch: 70, loss = 0.2780\n",
      "epoch: 80, loss = 0.2634\n",
      "epoch: 90, loss = 0.2510\n",
      "epoch: 100, loss = 0.2405\n",
      "accuracy = 0.9386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1. model\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        predicted = torch.sigmoid(self.linear(x))\n",
    "        return predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #updates\n",
    "    optimizer.step()\n",
    "    #zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset \n",
    "\n",
    "class TitanicData(Dataset):\n",
    "    def __init__(self, pre_process=None):\n",
    "        super(TitanicData, self).__init__()\n",
    "        data = pd.read_csv('./data/titanic.csv')\n",
    "        if pre_process:\n",
    "            self.features = PreProcess()\n",
    "\n",
    "        #self.features = data.drop('Survived', axis=1).to_numpy()\n",
    "        self.target = data[['Survived']].to_numpy()\n",
    "        \n",
    "        self.target_col = 'Survived'\n",
    "        self.feature_cols = [col for col in data.columns if col != self.target_col]\n",
    "        self.pre_process = pre_process\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.features[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.features[index], self.target[index]\n",
    "        if self.pre_process:\n",
    "            sample = self.pre_process(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class PreProcess:\n",
    "    def __call__(self, df):\n",
    "        return df.iloc[:,[0,1,5,6,7]]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = TitanicData(pre_process=PreProcess())\n",
    "\n",
    "#data_loader = DataLoader(dataset=my_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# total_samples = len(my_data)\n",
    "# n_iterations = total_sample/batch_size\n",
    "#for epoch in range(num_epochs):\n",
    "#   for i, (inputs, labels) in enumerate(data_loader):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PreProcess at 0x23d66d41730>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "#dataset = torchvision.datasets.MNIST(root='MNIST/raw/train-images-idx3-ubyte' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions \n",
    "* Apply a non-linear transformation and decide whether a neuron should be activated or not.\n",
    "\n",
    "1. Step function \n",
    "2. Sigmoid \n",
    "3. Tanh\n",
    "4. RelU\n",
    "5. Leaky ReLU -- modified version of relu to update the weights better\n",
    "6. Softmax -- Good for multi class classification problems in last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task or in other words it allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
    "\n",
    "For Eg:\n",
    "- we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
    "\n",
    "- Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
    "\n",
    "*******************************\n",
    "\n",
    "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
    "\n",
    "And there are several places you can find pretrained models to use for your own problems.\n",
    "\n",
    "\n",
    "\n",
    "| Location | what's there ? | Link(s)|\n",
    "|----|----|-----|\n",
    "|PyTorch domain libraries | Each of the PyTorch domain libraries (torchvision, torchtext) come with pretrained models of some form. The models there work right within PyTorch. | torchvision.models, torchtext.models, torchaudio.models, torchrec.models\n",
    "|HuggingFace Hub |A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. |\thttps://huggingface.co/models, https://huggingface.co/datasets\n",
    "|timm (PyTorch Image Models) library |\tAlmost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. |\thttps://github.com/rwightman/pytorch-image-models\n",
    "|Paperswithcode |\tA collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. \t|https://paperswithcode.com/|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/window/messmast.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metmast \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/window/messmast.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m turbine \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/window/turbine_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZeitstempel (UTC)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(metmast, turbine[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mws_2\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[0;32m     52\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    223\u001b[0m         src,\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    225\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    226\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    227\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    228\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    229\u001b[0m         errors\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    230\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    698\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    701\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    703\u001b[0m             handle,\n\u001b[0;32m    704\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    705\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    706\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    707\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    708\u001b[0m         )\n\u001b[0;32m    709\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    711\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/window/messmast.csv'"
     ]
    }
   ],
   "source": [
    "metmast = pd.read_csv('./data/window/messmast.csv', sep=',')\n",
    "turbine = pd.read_csv('./data/window/turbine_data.csv', sep=',', index_col='Zeitstempel (UTC)')\n",
    "data = pd.merge(metmast, turbine[['ws_2']], left_index=True, right_index=True )\n",
    "train = torch.tensor(data.iloc[:,:-1])\n",
    "test = torch.tensor(data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metmast['Date/time'] = pd.to_datetime(metmast['Date/time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatdata(df):\n",
    "        \n",
    "        df = df.drop(columns='Unnamed: 0', axis=1)#,inplace=True)\n",
    "        #self.data = self.data.set_index('Date/time')#, inplace = True)\n",
    "        \n",
    "        df['Date/time'] = pd.to_datetime(df['Date/time'])\n",
    "        df = df.set_index(df['Date/time'])\n",
    "        df = df.drop('Date/time', axis=1)\n",
    "        \n",
    "        df = df.replace(r'\\W', np.NaN, regex=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        data = df.rename(columns={'VZ1-03210099;wind_speed;Avg':'ws_142', 'V01-11208098;wind_speed;Avg':'ws_136',\n",
    "                   'V02-11208099;wind_speed;Avg':'ws_119','V03-11208100;wind_speed;Avg':'ws_99',\n",
    "                   'V04-11208101;wind_speed;Avg':'ws_79','V05-11208102;wind_speed;Avg':'ws_59',\n",
    "                   'V06-11208103;wind_speed;Avg':'ws_39','VZ1-03210099;wind_direction;Avg':'wd_142', \n",
    "                   'D01-08170370;wind_direction;Avg':'wd_136', 'D02-08170369;wind_direction;Avg':'wd_79',\n",
    "                   'HT1-236609;humidity;Avg':'H_136', 'HT2-236616;humidity;Avg':'H_99', 'HT3-236615;humidity;Avg':'H_30',\n",
    "                   'VZ1-03210099;temperature;Avg':'Temp_142','HT1-236609;temperature;Avg':'Temp_136','HT2-236616;temperature;Avg':'Temp_99',\n",
    "                   'HT3-236615;temperature;Avg':'Temp_30','BP1-150931338;air_pressure;Avg':'Press_16','VZ1-03210099;wind_speed_vert;Avg':'ws_vert_142'\n",
    "                    })#, inplace=True)\n",
    "        data= data.loc[:,['ws_142','ws_136','ws_119','ws_99','ws_79','ws_59','ws_39','ws_vert_142',\n",
    "                                   'wd_142','wd_136','wd_79',\n",
    "                                   'H_136', 'H_99', 'H_30',\n",
    "                                   'Temp_142','Temp_136','Temp_99','Temp_30',\n",
    "                                   'Press_16']]\n",
    "        \n",
    "        data['ws_142'] = data['ws_142'].astype('float') \n",
    "        data['ws_vert_142'] = data['ws_vert_142'].astype('float') \n",
    "        data['wd_142'] = data['wd_142'].astype('float') \n",
    "        data['Temp_142'] = data['Temp_142'].astype('float')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_formatdata(metmast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ws_142</th>\n",
       "      <th>ws_136</th>\n",
       "      <th>ws_119</th>\n",
       "      <th>ws_99</th>\n",
       "      <th>ws_79</th>\n",
       "      <th>ws_59</th>\n",
       "      <th>ws_39</th>\n",
       "      <th>ws_vert_142</th>\n",
       "      <th>wd_142</th>\n",
       "      <th>wd_136</th>\n",
       "      <th>wd_79</th>\n",
       "      <th>H_136</th>\n",
       "      <th>H_99</th>\n",
       "      <th>H_30</th>\n",
       "      <th>Temp_142</th>\n",
       "      <th>Temp_136</th>\n",
       "      <th>Temp_99</th>\n",
       "      <th>Temp_30</th>\n",
       "      <th>Press_16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date/time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-12 11:40:00</th>\n",
       "      <td>3.931345</td>\n",
       "      <td>3.5941</td>\n",
       "      <td>3.4347</td>\n",
       "      <td>3.4370</td>\n",
       "      <td>3.5016</td>\n",
       "      <td>3.1546</td>\n",
       "      <td>3.0875</td>\n",
       "      <td>0.860156</td>\n",
       "      <td>312.732139</td>\n",
       "      <td>322.3690</td>\n",
       "      <td>327.7007</td>\n",
       "      <td>88.0372</td>\n",
       "      <td>76.3993</td>\n",
       "      <td>63.7943</td>\n",
       "      <td>2.383594</td>\n",
       "      <td>0.64610</td>\n",
       "      <td>1.29480</td>\n",
       "      <td>2.87416</td>\n",
       "      <td>965.7203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 11:50:00</th>\n",
       "      <td>5.701858</td>\n",
       "      <td>5.7110</td>\n",
       "      <td>5.4175</td>\n",
       "      <td>5.0774</td>\n",
       "      <td>4.7813</td>\n",
       "      <td>4.1719</td>\n",
       "      <td>3.3333</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>305.892721</td>\n",
       "      <td>317.0449</td>\n",
       "      <td>319.0200</td>\n",
       "      <td>84.9587</td>\n",
       "      <td>72.7240</td>\n",
       "      <td>61.7138</td>\n",
       "      <td>2.187667</td>\n",
       "      <td>0.78178</td>\n",
       "      <td>1.41981</td>\n",
       "      <td>2.92920</td>\n",
       "      <td>965.7775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 12:00:00</th>\n",
       "      <td>4.890241</td>\n",
       "      <td>4.9387</td>\n",
       "      <td>4.8586</td>\n",
       "      <td>4.7327</td>\n",
       "      <td>4.4761</td>\n",
       "      <td>3.9801</td>\n",
       "      <td>3.3026</td>\n",
       "      <td>-0.249167</td>\n",
       "      <td>310.476240</td>\n",
       "      <td>322.0373</td>\n",
       "      <td>318.0044</td>\n",
       "      <td>79.9982</td>\n",
       "      <td>66.7090</td>\n",
       "      <td>55.3366</td>\n",
       "      <td>2.192500</td>\n",
       "      <td>0.82740</td>\n",
       "      <td>1.49020</td>\n",
       "      <td>3.11732</td>\n",
       "      <td>965.7703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 12:10:00</th>\n",
       "      <td>3.203742</td>\n",
       "      <td>3.2559</td>\n",
       "      <td>3.3009</td>\n",
       "      <td>3.3110</td>\n",
       "      <td>3.2263</td>\n",
       "      <td>2.9706</td>\n",
       "      <td>2.5173</td>\n",
       "      <td>0.271619</td>\n",
       "      <td>297.163956</td>\n",
       "      <td>309.3719</td>\n",
       "      <td>308.7647</td>\n",
       "      <td>78.5538</td>\n",
       "      <td>65.8790</td>\n",
       "      <td>53.6178</td>\n",
       "      <td>2.417028</td>\n",
       "      <td>1.09967</td>\n",
       "      <td>1.66077</td>\n",
       "      <td>3.27193</td>\n",
       "      <td>965.7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12 12:20:00</th>\n",
       "      <td>2.985836</td>\n",
       "      <td>2.8777</td>\n",
       "      <td>2.8353</td>\n",
       "      <td>2.6931</td>\n",
       "      <td>2.6192</td>\n",
       "      <td>2.4969</td>\n",
       "      <td>2.2133</td>\n",
       "      <td>0.687952</td>\n",
       "      <td>311.173388</td>\n",
       "      <td>323.2776</td>\n",
       "      <td>321.6449</td>\n",
       "      <td>76.4070</td>\n",
       "      <td>65.3727</td>\n",
       "      <td>52.6634</td>\n",
       "      <td>2.703442</td>\n",
       "      <td>1.42088</td>\n",
       "      <td>1.96130</td>\n",
       "      <td>3.60125</td>\n",
       "      <td>965.7871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ws_142  ws_136  ws_119   ws_99   ws_79   ws_59   ws_39  \\\n",
       "Date/time                                                                       \n",
       "2021-04-12 11:40:00  3.931345  3.5941  3.4347  3.4370  3.5016  3.1546  3.0875   \n",
       "2021-04-12 11:50:00  5.701858  5.7110  5.4175  5.0774  4.7813  4.1719  3.3333   \n",
       "2021-04-12 12:00:00  4.890241  4.9387  4.8586  4.7327  4.4761  3.9801  3.3026   \n",
       "2021-04-12 12:10:00  3.203742  3.2559  3.3009  3.3110  3.2263  2.9706  2.5173   \n",
       "2021-04-12 12:20:00  2.985836  2.8777  2.8353  2.6931  2.6192  2.4969  2.2133   \n",
       "\n",
       "                     ws_vert_142      wd_142    wd_136     wd_79    H_136  \\\n",
       "Date/time                                                                   \n",
       "2021-04-12 11:40:00     0.860156  312.732139  322.3690  327.7007  88.0372   \n",
       "2021-04-12 11:50:00     0.189500  305.892721  317.0449  319.0200  84.9587   \n",
       "2021-04-12 12:00:00    -0.249167  310.476240  322.0373  318.0044  79.9982   \n",
       "2021-04-12 12:10:00     0.271619  297.163956  309.3719  308.7647  78.5538   \n",
       "2021-04-12 12:20:00     0.687952  311.173388  323.2776  321.6449  76.4070   \n",
       "\n",
       "                        H_99     H_30  Temp_142  Temp_136  Temp_99  Temp_30  \\\n",
       "Date/time                                                                     \n",
       "2021-04-12 11:40:00  76.3993  63.7943  2.383594   0.64610  1.29480  2.87416   \n",
       "2021-04-12 11:50:00  72.7240  61.7138  2.187667   0.78178  1.41981  2.92920   \n",
       "2021-04-12 12:00:00  66.7090  55.3366  2.192500   0.82740  1.49020  3.11732   \n",
       "2021-04-12 12:10:00  65.8790  53.6178  2.417028   1.09967  1.66077  3.27193   \n",
       "2021-04-12 12:20:00  65.3727  52.6634  2.703442   1.42088  1.96130  3.60125   \n",
       "\n",
       "                     Press_16  \n",
       "Date/time                      \n",
       "2021-04-12 11:40:00  965.7203  \n",
       "2021-04-12 11:50:00  965.7775  \n",
       "2021-04-12 12:00:00  965.7703  \n",
       "2021-04-12 12:10:00  965.7675  \n",
       "2021-04-12 12:20:00  965.7871  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date/time</th>\n",
       "      <th>ws_142</th>\n",
       "      <th>ws_136</th>\n",
       "      <th>ws_119</th>\n",
       "      <th>ws_99</th>\n",
       "      <th>ws_79</th>\n",
       "      <th>ws_59</th>\n",
       "      <th>ws_39</th>\n",
       "      <th>ws_vert_142</th>\n",
       "      <th>wd_142</th>\n",
       "      <th>wd_136</th>\n",
       "      <th>wd_79</th>\n",
       "      <th>H_136</th>\n",
       "      <th>H_99</th>\n",
       "      <th>H_30</th>\n",
       "      <th>Temp_142</th>\n",
       "      <th>Temp_136</th>\n",
       "      <th>Temp_99</th>\n",
       "      <th>Temp_30</th>\n",
       "      <th>Press_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-12 11:40:00</td>\n",
       "      <td>3.931345</td>\n",
       "      <td>3.5941</td>\n",
       "      <td>3.4347</td>\n",
       "      <td>3.4370</td>\n",
       "      <td>3.5016</td>\n",
       "      <td>3.1546</td>\n",
       "      <td>3.0875</td>\n",
       "      <td>0.860156</td>\n",
       "      <td>312.732139</td>\n",
       "      <td>322.3690</td>\n",
       "      <td>327.7007</td>\n",
       "      <td>88.0372</td>\n",
       "      <td>76.3993</td>\n",
       "      <td>63.7943</td>\n",
       "      <td>2.383594</td>\n",
       "      <td>0.64610</td>\n",
       "      <td>1.29480</td>\n",
       "      <td>2.87416</td>\n",
       "      <td>965.7203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-12 11:50:00</td>\n",
       "      <td>5.701858</td>\n",
       "      <td>5.7110</td>\n",
       "      <td>5.4175</td>\n",
       "      <td>5.0774</td>\n",
       "      <td>4.7813</td>\n",
       "      <td>4.1719</td>\n",
       "      <td>3.3333</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>305.892721</td>\n",
       "      <td>317.0449</td>\n",
       "      <td>319.0200</td>\n",
       "      <td>84.9587</td>\n",
       "      <td>72.7240</td>\n",
       "      <td>61.7138</td>\n",
       "      <td>2.187667</td>\n",
       "      <td>0.78178</td>\n",
       "      <td>1.41981</td>\n",
       "      <td>2.92920</td>\n",
       "      <td>965.7775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-12 12:00:00</td>\n",
       "      <td>4.890241</td>\n",
       "      <td>4.9387</td>\n",
       "      <td>4.8586</td>\n",
       "      <td>4.7327</td>\n",
       "      <td>4.4761</td>\n",
       "      <td>3.9801</td>\n",
       "      <td>3.3026</td>\n",
       "      <td>-0.249167</td>\n",
       "      <td>310.476240</td>\n",
       "      <td>322.0373</td>\n",
       "      <td>318.0044</td>\n",
       "      <td>79.9982</td>\n",
       "      <td>66.7090</td>\n",
       "      <td>55.3366</td>\n",
       "      <td>2.192500</td>\n",
       "      <td>0.82740</td>\n",
       "      <td>1.49020</td>\n",
       "      <td>3.11732</td>\n",
       "      <td>965.7703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-12 12:10:00</td>\n",
       "      <td>3.203742</td>\n",
       "      <td>3.2559</td>\n",
       "      <td>3.3009</td>\n",
       "      <td>3.3110</td>\n",
       "      <td>3.2263</td>\n",
       "      <td>2.9706</td>\n",
       "      <td>2.5173</td>\n",
       "      <td>0.271619</td>\n",
       "      <td>297.163956</td>\n",
       "      <td>309.3719</td>\n",
       "      <td>308.7647</td>\n",
       "      <td>78.5538</td>\n",
       "      <td>65.8790</td>\n",
       "      <td>53.6178</td>\n",
       "      <td>2.417028</td>\n",
       "      <td>1.09967</td>\n",
       "      <td>1.66077</td>\n",
       "      <td>3.27193</td>\n",
       "      <td>965.7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-12 12:20:00</td>\n",
       "      <td>2.985836</td>\n",
       "      <td>2.8777</td>\n",
       "      <td>2.8353</td>\n",
       "      <td>2.6931</td>\n",
       "      <td>2.6192</td>\n",
       "      <td>2.4969</td>\n",
       "      <td>2.2133</td>\n",
       "      <td>0.687952</td>\n",
       "      <td>311.173388</td>\n",
       "      <td>323.2776</td>\n",
       "      <td>321.6449</td>\n",
       "      <td>76.4070</td>\n",
       "      <td>65.3727</td>\n",
       "      <td>52.6634</td>\n",
       "      <td>2.703442</td>\n",
       "      <td>1.42088</td>\n",
       "      <td>1.96130</td>\n",
       "      <td>3.60125</td>\n",
       "      <td>965.7871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date/time    ws_142  ws_136  ws_119   ws_99   ws_79   ws_59  \\\n",
       "0  2021-04-12 11:40:00  3.931345  3.5941  3.4347  3.4370  3.5016  3.1546   \n",
       "1  2021-04-12 11:50:00  5.701858  5.7110  5.4175  5.0774  4.7813  4.1719   \n",
       "2  2021-04-12 12:00:00  4.890241  4.9387  4.8586  4.7327  4.4761  3.9801   \n",
       "3  2021-04-12 12:10:00  3.203742  3.2559  3.3009  3.3110  3.2263  2.9706   \n",
       "4  2021-04-12 12:20:00  2.985836  2.8777  2.8353  2.6931  2.6192  2.4969   \n",
       "\n",
       "    ws_39  ws_vert_142      wd_142    wd_136     wd_79    H_136     H_99  \\\n",
       "0  3.0875     0.860156  312.732139  322.3690  327.7007  88.0372  76.3993   \n",
       "1  3.3333     0.189500  305.892721  317.0449  319.0200  84.9587  72.7240   \n",
       "2  3.3026    -0.249167  310.476240  322.0373  318.0044  79.9982  66.7090   \n",
       "3  2.5173     0.271619  297.163956  309.3719  308.7647  78.5538  65.8790   \n",
       "4  2.2133     0.687952  311.173388  323.2776  321.6449  76.4070  65.3727   \n",
       "\n",
       "      H_30  Temp_142  Temp_136  Temp_99  Temp_30  Press_16  \n",
       "0  63.7943  2.383594   0.64610  1.29480  2.87416  965.7203  \n",
       "1  61.7138  2.187667   0.78178  1.41981  2.92920  965.7775  \n",
       "2  55.3366  2.192500   0.82740  1.49020  3.11732  965.7703  \n",
       "3  53.6178  2.417028   1.09967  1.66077  3.27193  965.7675  \n",
       "4  52.6634  2.703442   1.42088  1.96130  3.60125  965.7871  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/window/metmast.csv', date_parser='Date/time' )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        metmast = pd.read_csv('./data/window/metmast.csv', sep=',')\n",
    "        metmast['Date/time'] = pd.to_datetime(metmast['Date/time'])\n",
    "        metmast.set_index('Date/time', inplace=True)\n",
    "        \n",
    "        turbine = pd.read_csv('./data/window/turbine_data.csv', sep=',')\n",
    "        turbine['Zeitstempel (UTC)'] = pd.to_datetime(turbine['Zeitstempel (UTC)'])\n",
    "        turbine = turbine.set_index(turbine['Zeitstempel (UTC)'])\n",
    "        turbine = turbine.drop('Zeitstempel (UTC)', axis=1)\n",
    "        \n",
    "        data = pd.merge(metmast, turbine[[target_col]], left_index=True, right_index=True )\n",
    "        self.data = data.dropna()\n",
    "        self.train = torch.tensor(data.iloc[:,:-1].values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(data.iloc[:,-1].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'x':self.train[idx, :], 'y':self.target[idx]}\n",
    "    \n",
    "    def train(self):\n",
    "        train_data = self.data[]\n",
    "\n",
    "\n",
    "dataset = Data(target_col='ws_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:len(dataset)-round(0.20*len(dataset))]\n",
    "test_data = dataset[len(dataset)-round(0.20*len(dataset)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Data"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25967"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(dataset)-round(0.20*len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (feature, label) in enumerate(train_data):\n",
    "    X = train_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = iter(train_data).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'x': tensor([[ 5.6788e+00,  5.8536e+00,  6.0008e+00,  6.1417e+00,  5.9840e+00,\n",
      "          5.3812e+00,  3.7216e+00,  2.0667e-02,  7.1468e+01,  8.1371e+01,\n",
      "          7.8063e+01,  5.3371e+01,  5.3916e+01,  5.8906e+01,  2.0382e+01,\n",
      "          1.8733e+01,  1.8917e+01,  1.7727e+01,  9.6228e+02],\n",
      "        [ 3.6791e+00,  3.6465e+00,  3.3282e+00,  2.8644e+00,  2.3667e+00,\n",
      "          2.1870e+00,  1.6848e+00,  5.5556e-02,  3.0561e+02,  3.1592e+02,\n",
      "          3.1046e+02,  1.0000e+02,  1.0000e+02,  1.0000e+02,  1.1870e+01,\n",
      "          9.9179e+00,  1.0234e+01,  1.0799e+01,  9.5785e+02],\n",
      "        [ 3.5830e+00,  3.5727e+00,  3.5508e+00,  3.3786e+00,  3.2982e+00,\n",
      "          3.1347e+00,  2.5237e+00, -1.1383e-01,  1.5840e+02,  1.6848e+02,\n",
      "          1.6614e+02,  6.8289e+01,  6.6651e+01,  6.5448e+01,  2.1532e+01,\n",
      "          1.9891e+01,  2.0577e+01,  2.1342e+01,  9.6557e+02],\n",
      "        [ 6.4069e+00,  6.3611e+00,  6.1493e+00,  5.7341e+00,  5.1858e+00,\n",
      "          4.1103e+00,  3.5190e+00,  3.2300e-01,  2.6586e+02,  2.7665e+02,\n",
      "          2.7753e+02,  6.6890e+01,  6.6298e+01,  6.2709e+01,  2.2088e+01,\n",
      "          2.0483e+01,  2.0981e+01,  2.2412e+01,  9.6437e+02],\n",
      "        [ 6.8572e+00,  6.7589e+00,  6.1734e+00,  5.3917e+00,  4.6333e+00,\n",
      "          3.8167e+00,  2.5665e+00,  4.3000e-02,  2.2621e+02,  2.3657e+02,\n",
      "          2.1718e+02,  7.1555e+01,  7.5769e+01,  7.7894e+01,  2.1475e+01,\n",
      "          1.9333e+01,  1.9214e+01,  1.8964e+01,  9.5490e+02]]), 'y': tensor([3.4000, 4.5000, 7.5000, 1.8000, 5.2000])}\n"
     ]
    }
   ],
   "source": [
    "for id, batch in enumerate(train_data):\n",
    "    print(id)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6788e+00,  5.8536e+00,  6.0008e+00,  6.1417e+00,  5.9840e+00,\n",
       "          5.3812e+00,  3.7216e+00,  2.0667e-02,  7.1468e+01,  8.1371e+01,\n",
       "          7.8063e+01,  5.3371e+01,  5.3916e+01,  5.8906e+01,  2.0382e+01,\n",
       "          1.8733e+01,  1.8917e+01,  1.7727e+01,  9.6228e+02],\n",
       "        [ 3.6791e+00,  3.6465e+00,  3.3282e+00,  2.8644e+00,  2.3667e+00,\n",
       "          2.1870e+00,  1.6848e+00,  5.5556e-02,  3.0561e+02,  3.1592e+02,\n",
       "          3.1046e+02,  1.0000e+02,  1.0000e+02,  1.0000e+02,  1.1870e+01,\n",
       "          9.9179e+00,  1.0234e+01,  1.0799e+01,  9.5785e+02],\n",
       "        [ 3.5830e+00,  3.5727e+00,  3.5508e+00,  3.3786e+00,  3.2982e+00,\n",
       "          3.1347e+00,  2.5237e+00, -1.1383e-01,  1.5840e+02,  1.6848e+02,\n",
       "          1.6614e+02,  6.8289e+01,  6.6651e+01,  6.5448e+01,  2.1532e+01,\n",
       "          1.9891e+01,  2.0577e+01,  2.1342e+01,  9.6557e+02],\n",
       "        [ 6.4069e+00,  6.3611e+00,  6.1493e+00,  5.7341e+00,  5.1858e+00,\n",
       "          4.1103e+00,  3.5190e+00,  3.2300e-01,  2.6586e+02,  2.7665e+02,\n",
       "          2.7753e+02,  6.6890e+01,  6.6298e+01,  6.2709e+01,  2.2088e+01,\n",
       "          2.0483e+01,  2.0981e+01,  2.2412e+01,  9.6437e+02],\n",
       "        [ 6.8572e+00,  6.7589e+00,  6.1734e+00,  5.3917e+00,  4.6333e+00,\n",
       "          3.8167e+00,  2.5665e+00,  4.3000e-02,  2.2621e+02,  2.3657e+02,\n",
       "          2.1718e+02,  7.1555e+01,  7.5769e+01,  7.7894e+01,  2.1475e+01,\n",
       "          1.9333e+01,  1.9214e+01,  1.8964e+01,  9.5490e+02]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, step 100/812, loss = 6.3602\n",
      "epoch 1 / 20, step 200/812, loss = 3.5926\n",
      "epoch 1 / 20, step 300/812, loss = 5.9049\n",
      "epoch 1 / 20, step 400/812, loss = 2.2060\n",
      "epoch 1 / 20, step 500/812, loss = 3.9939\n",
      "epoch 1 / 20, step 600/812, loss = 3.8563\n",
      "epoch 1 / 20, step 700/812, loss = 6.3894\n",
      "epoch 1 / 20, step 800/812, loss = 4.2226\n",
      "epoch 2 / 20, step 100/812, loss = 5.3900\n",
      "epoch 2 / 20, step 200/812, loss = 6.3230\n",
      "epoch 2 / 20, step 300/812, loss = 6.9829\n",
      "epoch 2 / 20, step 400/812, loss = 5.0459\n",
      "epoch 2 / 20, step 500/812, loss = 8.2923\n",
      "epoch 2 / 20, step 600/812, loss = 5.4046\n",
      "epoch 2 / 20, step 700/812, loss = 4.9760\n",
      "epoch 2 / 20, step 800/812, loss = 5.5554\n",
      "epoch 3 / 20, step 100/812, loss = 10.7448\n",
      "epoch 3 / 20, step 200/812, loss = 3.6052\n",
      "epoch 3 / 20, step 300/812, loss = 4.5006\n",
      "epoch 3 / 20, step 400/812, loss = 3.8603\n",
      "epoch 3 / 20, step 500/812, loss = 3.2604\n",
      "epoch 3 / 20, step 600/812, loss = 5.5999\n",
      "epoch 3 / 20, step 700/812, loss = 5.1182\n",
      "epoch 3 / 20, step 800/812, loss = 2.1629\n",
      "epoch 4 / 20, step 100/812, loss = 6.3197\n",
      "epoch 4 / 20, step 200/812, loss = 8.3132\n",
      "epoch 4 / 20, step 300/812, loss = 3.1056\n",
      "epoch 4 / 20, step 400/812, loss = 5.6046\n",
      "epoch 4 / 20, step 500/812, loss = 3.8641\n",
      "epoch 4 / 20, step 600/812, loss = 2.8859\n",
      "epoch 4 / 20, step 700/812, loss = 4.5699\n",
      "epoch 4 / 20, step 800/812, loss = 4.8518\n",
      "epoch 5 / 20, step 100/812, loss = 3.8618\n",
      "epoch 5 / 20, step 200/812, loss = 6.8739\n",
      "epoch 5 / 20, step 300/812, loss = 4.5720\n",
      "epoch 5 / 20, step 400/812, loss = 7.1133\n",
      "epoch 5 / 20, step 500/812, loss = 8.3508\n",
      "epoch 5 / 20, step 600/812, loss = 3.8877\n",
      "epoch 5 / 20, step 700/812, loss = 5.4700\n",
      "epoch 5 / 20, step 800/812, loss = 4.1519\n",
      "epoch 6 / 20, step 100/812, loss = 4.5691\n",
      "epoch 6 / 20, step 200/812, loss = 4.1675\n",
      "epoch 6 / 20, step 300/812, loss = 3.8770\n",
      "epoch 6 / 20, step 400/812, loss = 4.2074\n",
      "epoch 6 / 20, step 500/812, loss = 4.4265\n",
      "epoch 6 / 20, step 600/812, loss = 2.4337\n",
      "epoch 6 / 20, step 700/812, loss = 3.6311\n",
      "epoch 6 / 20, step 800/812, loss = 3.3718\n",
      "epoch 7 / 20, step 100/812, loss = 4.7607\n",
      "epoch 7 / 20, step 200/812, loss = 4.1391\n",
      "epoch 7 / 20, step 300/812, loss = 4.6913\n",
      "epoch 7 / 20, step 400/812, loss = 6.3916\n",
      "epoch 7 / 20, step 500/812, loss = 4.4087\n",
      "epoch 7 / 20, step 600/812, loss = 7.4975\n",
      "epoch 7 / 20, step 700/812, loss = 3.7961\n",
      "epoch 7 / 20, step 800/812, loss = 3.4478\n",
      "epoch 8 / 20, step 100/812, loss = 3.2505\n",
      "epoch 8 / 20, step 200/812, loss = 1.9311\n",
      "epoch 8 / 20, step 300/812, loss = 3.6368\n",
      "epoch 8 / 20, step 400/812, loss = 3.9688\n",
      "epoch 8 / 20, step 500/812, loss = 9.5179\n",
      "epoch 8 / 20, step 600/812, loss = 4.5408\n",
      "epoch 8 / 20, step 700/812, loss = 5.5885\n",
      "epoch 8 / 20, step 800/812, loss = 8.9628\n",
      "epoch 9 / 20, step 100/812, loss = 7.9354\n",
      "epoch 9 / 20, step 200/812, loss = 5.5690\n",
      "epoch 9 / 20, step 300/812, loss = 5.1946\n",
      "epoch 9 / 20, step 400/812, loss = 3.7658\n",
      "epoch 9 / 20, step 500/812, loss = 7.3694\n",
      "epoch 9 / 20, step 600/812, loss = 4.2793\n",
      "epoch 9 / 20, step 700/812, loss = 2.6116\n",
      "epoch 9 / 20, step 800/812, loss = 4.7617\n",
      "epoch 10 / 20, step 100/812, loss = 5.6063\n",
      "epoch 10 / 20, step 200/812, loss = 3.6814\n",
      "epoch 10 / 20, step 300/812, loss = 4.9697\n",
      "epoch 10 / 20, step 400/812, loss = 3.8877\n",
      "epoch 10 / 20, step 500/812, loss = 3.1226\n",
      "epoch 10 / 20, step 600/812, loss = 5.3890\n",
      "epoch 10 / 20, step 700/812, loss = 4.5176\n",
      "epoch 10 / 20, step 800/812, loss = 4.4072\n",
      "epoch 11 / 20, step 100/812, loss = 4.0907\n",
      "epoch 11 / 20, step 200/812, loss = 4.0289\n",
      "epoch 11 / 20, step 300/812, loss = 3.1164\n",
      "epoch 11 / 20, step 400/812, loss = 4.8904\n",
      "epoch 11 / 20, step 500/812, loss = 4.2155\n",
      "epoch 11 / 20, step 600/812, loss = 5.2431\n",
      "epoch 11 / 20, step 700/812, loss = 4.0057\n",
      "epoch 11 / 20, step 800/812, loss = 4.6819\n",
      "epoch 12 / 20, step 100/812, loss = 5.1284\n",
      "epoch 12 / 20, step 200/812, loss = 3.9170\n",
      "epoch 12 / 20, step 300/812, loss = 3.7712\n",
      "epoch 12 / 20, step 400/812, loss = 5.7844\n",
      "epoch 12 / 20, step 500/812, loss = 4.7474\n",
      "epoch 12 / 20, step 600/812, loss = 5.6313\n",
      "epoch 12 / 20, step 700/812, loss = 3.7140\n",
      "epoch 12 / 20, step 800/812, loss = 5.1994\n",
      "epoch 13 / 20, step 100/812, loss = 4.6276\n",
      "epoch 13 / 20, step 200/812, loss = 6.6080\n",
      "epoch 13 / 20, step 300/812, loss = 5.3731\n",
      "epoch 13 / 20, step 400/812, loss = 3.9405\n",
      "epoch 13 / 20, step 500/812, loss = 5.0035\n",
      "epoch 13 / 20, step 600/812, loss = 5.4080\n",
      "epoch 13 / 20, step 700/812, loss = 3.5755\n",
      "epoch 13 / 20, step 800/812, loss = 5.8343\n",
      "epoch 14 / 20, step 100/812, loss = 3.6279\n",
      "epoch 14 / 20, step 200/812, loss = 4.3412\n",
      "epoch 14 / 20, step 300/812, loss = 3.0368\n",
      "epoch 14 / 20, step 400/812, loss = 2.1932\n",
      "epoch 14 / 20, step 500/812, loss = 2.3383\n",
      "epoch 14 / 20, step 600/812, loss = 4.9719\n",
      "epoch 14 / 20, step 700/812, loss = 2.7402\n",
      "epoch 14 / 20, step 800/812, loss = 7.9914\n",
      "epoch 15 / 20, step 100/812, loss = 4.3358\n",
      "epoch 15 / 20, step 200/812, loss = 6.5940\n",
      "epoch 15 / 20, step 300/812, loss = 4.0871\n",
      "epoch 15 / 20, step 400/812, loss = 1.9720\n",
      "epoch 15 / 20, step 500/812, loss = 5.9104\n",
      "epoch 15 / 20, step 600/812, loss = 5.9715\n",
      "epoch 15 / 20, step 700/812, loss = 4.5799\n",
      "epoch 15 / 20, step 800/812, loss = 9.2253\n",
      "epoch 16 / 20, step 100/812, loss = 4.6668\n",
      "epoch 16 / 20, step 200/812, loss = 4.1164\n",
      "epoch 16 / 20, step 300/812, loss = 3.5170\n",
      "epoch 16 / 20, step 400/812, loss = 4.9226\n",
      "epoch 16 / 20, step 500/812, loss = 3.9191\n",
      "epoch 16 / 20, step 600/812, loss = 5.0679\n",
      "epoch 16 / 20, step 700/812, loss = 3.5472\n",
      "epoch 16 / 20, step 800/812, loss = 4.0236\n",
      "epoch 17 / 20, step 100/812, loss = 4.2399\n",
      "epoch 17 / 20, step 200/812, loss = 4.1895\n",
      "epoch 17 / 20, step 300/812, loss = 4.4591\n",
      "epoch 17 / 20, step 400/812, loss = 3.6074\n",
      "epoch 17 / 20, step 500/812, loss = 4.1808\n",
      "epoch 17 / 20, step 600/812, loss = 4.9832\n",
      "epoch 17 / 20, step 700/812, loss = 6.5787\n",
      "epoch 17 / 20, step 800/812, loss = 2.8780\n",
      "epoch 18 / 20, step 100/812, loss = 5.4520\n",
      "epoch 18 / 20, step 200/812, loss = 3.3931\n",
      "epoch 18 / 20, step 300/812, loss = 4.0982\n",
      "epoch 18 / 20, step 400/812, loss = 4.5543\n",
      "epoch 18 / 20, step 500/812, loss = 4.8388\n",
      "epoch 18 / 20, step 600/812, loss = 3.6601\n",
      "epoch 18 / 20, step 700/812, loss = 7.5077\n",
      "epoch 18 / 20, step 800/812, loss = 3.3986\n",
      "epoch 19 / 20, step 100/812, loss = 4.3588\n",
      "epoch 19 / 20, step 200/812, loss = 5.8240\n",
      "epoch 19 / 20, step 300/812, loss = 5.0787\n",
      "epoch 19 / 20, step 400/812, loss = 2.6621\n",
      "epoch 19 / 20, step 500/812, loss = 2.4736\n",
      "epoch 19 / 20, step 600/812, loss = 3.1326\n",
      "epoch 19 / 20, step 700/812, loss = 3.9813\n",
      "epoch 19 / 20, step 800/812, loss = 6.5961\n",
      "epoch 20 / 20, step 100/812, loss = 4.3036\n",
      "epoch 20 / 20, step 200/812, loss = 4.0676\n",
      "epoch 20 / 20, step 300/812, loss = 5.1543\n",
      "epoch 20 / 20, step 400/812, loss = 4.4044\n",
      "epoch 20 / 20, step 500/812, loss = 3.8071\n",
      "epoch 20 / 20, step 600/812, loss = 3.1680\n",
      "epoch 20 / 20, step 700/812, loss = 3.9173\n",
      "epoch 20 / 20, step 800/812, loss = 2.9577\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, target_col):\n",
    "        metmast = pd.read_csv('./data/window/metmast.csv', sep=',')\n",
    "        metmast['Date/time'] = pd.to_datetime(metmast['Date/time'])\n",
    "        metmast.set_index('Date/time', inplace=True)\n",
    "        \n",
    "        turbine = pd.read_csv('./data/window/turbine_data.csv', sep=',')\n",
    "        turbine['Zeitstempel (UTC)'] = pd.to_datetime(turbine['Zeitstempel (UTC)'])\n",
    "        turbine = turbine.set_index(turbine['Zeitstempel (UTC)'])\n",
    "        turbine = turbine.drop('Zeitstempel (UTC)', axis=1)\n",
    "        \n",
    "        data = pd.merge(metmast, turbine[[target_col]], left_index=True, right_index=True )\n",
    "        data = data.dropna()\n",
    "        self.train = torch.tensor(data.iloc[:,:-1].values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(data.iloc[:,-1].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.train[idx, :], self.target[idx]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 80),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(80, 1)                     \n",
    "        )\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.base(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "dataset = Data(target_col='ws_2')\n",
    "\n",
    "train_size = len(dataset)-round(0.20*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(train_dataset, batch_size=32 , shuffle=True)\n",
    "test_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "# model\n",
    "model = Model(input_size=19)\n",
    "\n",
    "\n",
    "# loss and optimizer --> cross entropy\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "n_total_steps = len(train_dataset)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for id, (features, label) in enumerate(train_dataset):\n",
    "\n",
    "        features = features.to(device)\n",
    "        label = label.to(device)\n",
    "        #label = label.type(torch.LongTensor)\n",
    "        \n",
    "        # forward\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (id+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {id+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for id, sample in enumerate(train_dataset):\n",
    "    print(type(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
